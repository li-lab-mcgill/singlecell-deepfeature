{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import GEOparse\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "\n",
    "from scvi.dataset import AnnDatasetFromAnnData\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import umap\n",
    "\n",
    "from utils_helper import *\n",
    "\n",
    "seed = 345\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "\n",
    "gpus = [\"1\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(gpus)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LoomDataset('path_to_dataset.loom')\n",
    "celltypes = pd.read_csv('/home/mcb/users/mbahra5/project/data/panc8/celltypes.csv')\n",
    "tech = pd.read_csv('/home/mcb/users/mbahra5/project/data/panc8/tech.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes = celltypes['x']\n",
    "tech = tech['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adata = anndata.AnnData(X=data.X)\n",
    "\n",
    "adata.obs['cell_types'] = celltypes.values\n",
    "adata.obs['cell_type'] = celltypes.values\n",
    "\n",
    "adata.obs['batch_name'] = tech.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[adata.obs['batch_name']!='indrop'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['labels'] = adata.obs['cell_types'].astype('category').cat.codes.values\n",
    "\n",
    "adata.obs['batch'] = adata.obs['batch_name'].astype('category').cat.codes.values\n",
    "adata.obs['batch_indices'] = adata.obs['batch'].values\n",
    "\n",
    "\n",
    "n_labels = len(adata.obs['cell_types'].unique())\n",
    "n_batch = len(adata.obs['batch'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.subsample(adata,fraction=1,random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AnnDatasetFromAnnData(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "eps = 1e-8\n",
    "use_batches = True\n",
    "use_cuda = True\n",
    "n_latent = 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(dataset.nb_genes, n_batch=dataset.n_batches * use_batches, n_latent=n_latent, n_layers = 2, n_hidden=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(n_latent, [2*n_latent], n_batch).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GANTrainer(\n",
    "    vae, \n",
    "    disc,\n",
    "    dataset,\n",
    "    train_size=0.9999999999999,  test_size=None,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=5,\n",
    "    seed = seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(n_epochs=20, lr= lr*1, eps=eps, disc_lr= lr * 1, enc_lr = lr* 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(n_epochs=50, lr= lr*1, eps=eps, disc_lr= lr * 1, enc_lr = lr* 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_train = history[0]\n",
    "x = np.linspace(0, len(elbo_train), len(elbo_train))\n",
    "plt.plot(x, elbo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = trainer.create_posterior(trainer.model, dataset, indices=np.arange(len(dataset)))\n",
    "latent, batches, labels = posterior.sequential().get_latent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adata.obsm[\"X_scVI\"] = latent\n",
    "latent_tensor = torch.tensor(latent, device='cuda:0')\n",
    "batch_prediction = np.argmax(disc(latent_tensor).detach().cpu().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Scores by kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(input_posterior):\n",
    "    latent, batches, labels = input_posterior.sequential().get_latent()\n",
    "    print(\"Entropy of batch mixing :\", entropy_batch_mixing(latent,batches))\n",
    "    print(\"Clustering ARI = {}\".format(clustering_scores(dataset.n_labels, labels, latent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Set:')\n",
    "calc_scores(trainer.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Set:')\n",
    "calc_scores(trainer.test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation Set:')\n",
    "calc_scores(trainer.validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.tsne(adata, use_rep='X_scVI', n_pcs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['batch_cat'] = adata.obs['batch'].astype('category')\n",
    "adata.obs['batch_pred_cat'] = pd.Series(batch_prediction).astype('category').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Human_Pancreatic_Cells'\n",
    "# method = 'scGAN'\n",
    "method = '$scGAN^{-}$(No Adversarial Net)'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6), dpi=150)\n",
    "sc.pl.tsne(adata, color=[\"cell_types\"], ax=ax, title='{} - Cell Type'.format(method))\n",
    "fig.savefig('/home/mcb/users/mbahra5/project/scVI/pics/{}_{}_celltype.png'.format(dataset_name,method), bbox_inches = 'tight')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6),  dpi=150)\n",
    "sc.pl.tsne(adata, color=[\"batch_name\"], ax=ax, title='{} - Batch'.format(method))\n",
    "fig.savefig('/home/mcb/users/mbahra5/project/scVI/pics/{}_{}_batch.png'.format(dataset_name,method), bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep=\"X_scVI\", n_neighbors=30)\n",
    "sc.tl.louvain(adata, resolution=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot = True\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "sc.pl.tsne(adata, color=['louvain'], ax=ax, show=show_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "ari_score = ARI(labels, adata.obs['louvain'])\n",
    "print(ari_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "sc.pp.neighbors(adata, use_rep=\"X_scVI\", n_neighbors=15)\n",
    "sc.tl.umap(adata, min_dist=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot = True\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sc.pl.umap(adata, color=[\"cell_type\"], ax=ax, show=show_plot)\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sc.pl.umap(adata, color=[\"batch_name\"], ax=ax, show=show_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.inference import UnsupervisedTrainer\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from tqdm import trange\n",
    "from scvi.inference.posterior import Posterior\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GANTrainer(UnsupervisedTrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        disc,\n",
    "        gene_dataset,\n",
    "        train_size, test_size,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.disc = disc\n",
    "                 \n",
    "        super().__init__(model, gene_dataset, train_size=train_size, test_size=test_size, **kwargs)\n",
    "        if type(self) is GANTrainer:\n",
    "            self.train_set, self.test_set, self.validation_set = self.train_test_validation(\n",
    "                model, gene_dataset, train_size, test_size\n",
    "            )\n",
    "        \n",
    "    def train(self, n_epochs=20, lr=1e-3, eps=0.01, params=None, enc_lr=1e-3, disc_lr=1e-3):\n",
    "        begin = time.time()\n",
    "        self.model.train()\n",
    "        self.disc.train()\n",
    "\n",
    "        if params is None:\n",
    "            params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "\n",
    "        optimizer = self.optimizer = torch.optim.Adam(params, lr=lr, eps=eps, weight_decay=self.weight_decay)\n",
    "        optimizerE = self.optimizerE = torch.optim.Adam(self.model.z_encoder.parameters(), lr = enc_lr, weight_decay=self.weight_decay)\n",
    "        optimizerD = self.optimizerD = torch.optim.Adam(self.disc.parameters(), lr = disc_lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        nll_loss = nn.NLLLoss(reduction='none') \n",
    "        kl_loss = nn.KLDivLoss()\n",
    "        mse_loss = nn.MSELoss()\n",
    "\n",
    "        with trange(n_epochs, desc=\"training\", file=sys.stdout, disable=not self.show_progbar) as pbar:\n",
    "            vae_loss_list, E_loss_list, D_loss_list = [], [], []\n",
    "            for self.epoch in pbar:\n",
    "                vae_loss_list_epoch, E_loss_list_epoch, D_loss_list_epoch = [], [], []\n",
    "                \n",
    "                pbar.update(1)\n",
    "                self.on_epoch_begin()\n",
    "\n",
    "    \n",
    "                for tensors_list in self.data_loaders_loop():\n",
    "                    if tensors_list[0][0].shape[0] < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    sample_batch, local_l_mean, local_l_var, batch_index, _ = tensors_list[0]  \n",
    "                    ############################\n",
    "                    # (1) Update VAE network\n",
    "                    ###########################                    \n",
    "                    self.model.zero_grad()\n",
    "                        \n",
    "                    reconst_loss, kl_divergence, z = self.model(sample_batch, local_l_mean, local_l_var, batch_index)\n",
    "                    loss = torch.mean(reconst_loss + self.kl_weight * kl_divergence)\n",
    "                    \n",
    "                    vae_loss_list_epoch.append(loss.item())\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "                    ############################\n",
    "                    # (1) Update D Net\n",
    "                    ###########################     \n",
    "                    for disc_iter in range(10):\n",
    "                        self.disc.zero_grad()\n",
    "\n",
    "                        batch_pred = self.disc(z)\n",
    "                        D_loss = nll_loss(batch_pred, batch_index.view(-1)) \n",
    "                        D_loss = torch.mean(D_loss) # todo\n",
    "#                         D_loss = mse_loss(batch_pred, batch_index.view(-1))\n",
    "                        D_loss_list_epoch.append(D_loss.item())\n",
    "                        D_loss.backward(retain_graph=True)\n",
    "                        optimizerD.step()\n",
    "                    ############################\n",
    "                    # (1) Update E Net\n",
    "                    ########################### \n",
    "                    self.model.z_encoder.zero_grad()\n",
    "                    E_loss = -1 * D_loss\n",
    "\n",
    "                    E_loss_list_epoch.append(E_loss.item())\n",
    "                    E_loss.backward(retain_graph=True)\n",
    "                    optimizerE.step()\n",
    "                    \n",
    "                vae_loss_list.append(sum(vae_loss_list_epoch)/len(vae_loss_list_epoch))\n",
    "                D_loss_list.append(sum(D_loss_list_epoch)/len(D_loss_list_epoch))\n",
    "                E_loss_list.append(sum(E_loss_list_epoch)/len(E_loss_list_epoch))\n",
    "\n",
    "\n",
    "        self.model.eval()\n",
    "        return vae_loss_list, D_loss_list, E_loss_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
